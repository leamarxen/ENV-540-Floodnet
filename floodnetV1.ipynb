{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloodNet Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "#import model.resnet as models\n",
    "\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## First Tries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image = Image.open(\"data/image/10165.jpg\")\n",
    "image\n",
    "np_image = np.array(image)\n",
    "print(np_image.shape)\n",
    "#print(np_image)\n",
    "print(np.sum(np_image)/np_image.shape[0]/np_image.shape[1]/np_image.shape[2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import csv\n",
    "#filename = open(\"FloodNet_split_train_valid_test.csv\", 'r')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"FloodNet_split_train_valid_test.csv\", sep=',', header=None, names=[\"Column1\", \"Column2\", \"Column3\"])\n",
    "df1 = df[df[\"Column3\"]=='train']\n",
    "df1[\"Column1\"].map(lambda x: \"data/\" + x)#.iloc[1]\n",
    "\n",
    "list(zip(df1[\"Column1\"], df1[\"Column2\"]))  \n",
    "df1['Column1']\n",
    "\n",
    "for entry in list(df['Column1']):\n",
    "    filepath = 'Data/Image/' + entry\n",
    "    image = Image.open(filepath)\n",
    "    np_image = np.array(image)\n",
    "    if np_image.shape[0] != 3000:\n",
    "        print(entry, np_image.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function\n",
    "#code in this cell from https://albumentations.ai/docs/examples/example_kaggle_salt/\n",
    "def visualize(image, mask, original_image=None, original_mask=None):\n",
    "    fontsize = 18\n",
    "    \n",
    "    if original_image is None and original_mask is None:\n",
    "        f, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "        ax[0].imshow(image)\n",
    "        ax[1].imshow(mask)\n",
    "    else:\n",
    "        f, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "        ax[0, 0].imshow(original_image)\n",
    "        ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
    "        \n",
    "        ax[1, 0].imshow(original_mask)\n",
    "        ax[1, 0].set_title('Original mask', fontsize=fontsize)\n",
    "        \n",
    "        ax[0, 1].imshow(image)\n",
    "        ax[0, 1].set_title('Transformed image', fontsize=fontsize)\n",
    "        \n",
    "        ax[1, 1].imshow(mask)\n",
    "        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloodData(Dataset):\n",
    "\n",
    "    # mapping between label class names and indices\n",
    "    LABEL_CLASSES = {\n",
    "      'background': \t\t  0,\n",
    "      'building-flooded': \t\t\t    1,\n",
    "      'building-non-flooded': \t  2,\n",
    "      'road-flooded': \t\t\t\t      3,\n",
    "      'road-non-flooded': \t\t\t    4,\n",
    "      'water': \t\t\t    5,\n",
    "      'tree':   6,\n",
    "      'vehicle': \t\t\t\t    7,\n",
    "      'pool': \t\t\t\t    8,\n",
    "      'grass': \t\t\t  9\n",
    "    }\n",
    "   \n",
    "\n",
    "    def __init__(self, transforms=None, split='train'):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        \n",
    "        SPLIT = pd.read_csv(\"FloodNet_split_train_valid_test.csv\", sep=',', header=None, names=[\"Column1\", \"Column2\", \"Column3\"])\n",
    "        SPLIT[\"Column1\"] = SPLIT[\"Column1\"].map(lambda x: \"Data/image/\" + x)\n",
    "        SPLIT[\"Column2\"] = SPLIT[\"Column2\"].map(lambda x: \"Data/mask/\" + x)\n",
    "        \n",
    "        splitted_set = SPLIT[SPLIT[\"Column3\"]==split]\n",
    "        \n",
    "        # prepare data\n",
    "        self.data = list(zip(splitted_set[\"Column1\"], splitted_set[\"Column2\"]))                                  # list of tuples of (image path, label class)\n",
    "        \"\"\"\n",
    "        images = np.empty((len(self.data)*3000,len(self.data)*4000,3))\n",
    "        \n",
    "        for i in range(len(self.data)):\n",
    "            images[i] = np.array()\n",
    "            Image.open()\n",
    "        \"\"\" \n",
    "            \n",
    "    #TODO: please provide the remaining functions required for the torch.utils.data.Dataset class.\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        imgName, labelsName = self.data[x]\n",
    "\n",
    "        img = np.array(Image.open(imgName))\n",
    "        labels = np.array(Image.open(labelsName))\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=img, mask=labels)\n",
    "            #code to visualize transformation - uncomment if want to use\n",
    "            #visualize(transformed[\"image\"], transformed[\"mask\"], img, labels)\n",
    "            img = transformed['image']\n",
    "            labels = transformed['mask']\n",
    "        else:\n",
    "            img, labels = img[:3000, :4000,:], labels[:3000, :4000]\n",
    "        \n",
    "        img, labels = torch.tensor(img, dtype=torch.double), torch.tensor(labels)\n",
    "        return img, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [19:53<00:00, 66.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([106.5385, 116.1601,  87.6059], dtype=torch.float64) tensor([53.1838, 49.5204, 53.5829], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# source of code in this cell:\n",
    "# https://www.binarystudy.com/2021/04/how-to-calculate-mean-standard-deviation-images-pytorch.html\n",
    "\n",
    "train_not_transformed_set = FloodData(transforms = None, split = 'train')\n",
    "train_not_transformed_loader = DataLoader(train_not_transformed_set, batch_size = 16)\n",
    "\n",
    "def batch_mean_and_sd(loader):\n",
    "    count = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "    \n",
    "    for images, _  in tqdm(loader):\n",
    "        b, h, w, c = images.shape #batch, height, width, color\n",
    "        nb_pixels = b*h*w\n",
    "        \n",
    "        sum_ = torch.sum(images, dim = [0,1,2])\n",
    "        sum_of_square = torch.sum(torch.square(images), dim = [0,1,2])\n",
    "        \n",
    "        fst_moment = (count*fst_moment+sum_)/(count+nb_pixels)\n",
    "        snd_moment = (count*snd_moment+sum_of_square)/(count+nb_pixels)\n",
    "\n",
    "        count += nb_pixels\n",
    "        \n",
    "    mean, std = fst_moment, torch.sqrt(snd_moment-fst_moment**2)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = batch_mean_and_sd(train_not_transformed_loader)\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = A.Compose([\n",
    "    A.RandomSizedCrop(min_max_height = [1000, 2500], height=713, width=713),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    #A.Blur(blur_limit = 3),\n",
    "    A.RandomRotate90(),\n",
    "    #A.OpticalDistortion(),\n",
    "    #A.GridDistortion(),\n",
    "    #A.Resize(height=713, width=713),\n",
    "    \n",
    "    #Normalization is applied by the formula: img = (img - mean * max_pixel_value) / (std * max_pixel_value)\n",
    "    A.Normalize(mean = mean, std = std, max_pixel_value=1)\n",
    "])\n",
    "\n",
    "transform_val = A.Compose([\n",
    "    A.RandomSizedCrop(min_max_height = [500, 2500], height=713, width=713),\n",
    "    #A.Resize(height=713, width=713),\n",
    "    A.Normalize(mean = mean, std = std, max_pixel_value=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FloodData(transforms = transform_train, split = 'train')\n",
    "val_dataset = FloodData(transforms = transform_val, split = 'valid')\n",
    "test_dataset = FloodData(transforms = transform_val, split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1215, -0.3667, -0.1606],\n",
       "          [ 0.0839, -0.3667, -0.1606],\n",
       "          [ 0.0651, -0.4071, -0.1793],\n",
       "          ...,\n",
       "          [ 0.6292,  0.0372,  0.0260],\n",
       "          [ 0.6480,  0.0977,  0.0820],\n",
       "          [ 0.5916,  0.0775,  0.0820]],\n",
       "\n",
       "         [[ 0.1403, -0.3465, -0.1233],\n",
       "          [ 0.1403, -0.3061, -0.0860],\n",
       "          [ 0.1027, -0.3667, -0.1419],\n",
       "          ...,\n",
       "          [ 0.6104,  0.0372,  0.0260],\n",
       "          [ 0.6292,  0.0977,  0.0820],\n",
       "          [ 0.4787, -0.0234, -0.0113]],\n",
       "\n",
       "         [[ 0.1403, -0.3465, -0.1046],\n",
       "          [ 0.1403, -0.3465, -0.1046],\n",
       "          [ 0.1215, -0.3465, -0.1233],\n",
       "          ...,\n",
       "          [ 0.5728,  0.0170,  0.0260],\n",
       "          [ 0.6104,  0.1179,  0.1193],\n",
       "          [ 0.4223, -0.0436, -0.0113]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5540, -0.0840, -0.1979],\n",
       "          [ 0.4035, -0.2456, -0.2912],\n",
       "          [ 0.4035, -0.2254, -0.2353],\n",
       "          ...,\n",
       "          [ 0.5164, -0.2254, -0.1979],\n",
       "          [ 0.5728, -0.1648, -0.1606],\n",
       "          [ 0.5352, -0.2052, -0.1793]],\n",
       "\n",
       "         [[ 0.5916, -0.0436, -0.1419],\n",
       "          [ 0.5352, -0.1042, -0.1606],\n",
       "          [ 0.4975, -0.1446, -0.1419],\n",
       "          ...,\n",
       "          [ 0.4223, -0.3061, -0.2539],\n",
       "          [ 0.5728, -0.1446, -0.1419],\n",
       "          [ 0.6104, -0.1042, -0.0860]],\n",
       "\n",
       "         [[ 0.6104, -0.0234, -0.0673],\n",
       "          [ 0.6480,  0.0372,  0.0074],\n",
       "          [ 0.5540, -0.0840, -0.0673],\n",
       "          ...,\n",
       "          [ 0.5352, -0.1850, -0.1419],\n",
       "          [ 0.6104, -0.1042, -0.1046],\n",
       "          [ 0.6104, -0.1042, -0.0860]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 1)\n",
    "next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, deep_base=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.deep_base = deep_base\n",
    "        if not self.deep_base:\n",
    "            self.inplanes = 64\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "        else:\n",
    "            self.inplanes = 128\n",
    "            self.conv1 = conv3x3(3, 64, stride=2)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            \n",
    "        #on a décalé\n",
    "        self.conv2 = conv3x3(64, 64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = conv3x3(64, 128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "            \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        if self.deep_base:\n",
    "            x = self.relu(self.bn2(self.conv2(x)))\n",
    "            x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPM(nn.Module):\n",
    "    def __init__(self, in_dim, reduction_dim, bins):\n",
    "        super(PPM, self).__init__()\n",
    "        self.features = []\n",
    "        for bin in bins:\n",
    "            self.features.append(nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(bin),\n",
    "                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(reduction_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        self.features = nn.ModuleList(self.features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_size = x.size()\n",
    "        out = [x]\n",
    "        for f in self.features:\n",
    "            out.append(F.interpolate(f(x), x_size[2:], mode='bilinear', align_corners=True))\n",
    "        return torch.cat(out, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSPNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, layers=50, bins=(1, 2, 3, 6), dropout=0.1, classes=10, zoom_factor=8, criterion=nn.CrossEntropyLoss(ignore_index=255), pretrained=True):\n",
    "        super(PSPNet, self).__init__()\n",
    "        assert layers in [50, 101, 152]\n",
    "        assert 2048 % len(bins) == 0\n",
    "        assert classes > 1\n",
    "        assert zoom_factor in [1, 2, 4, 8]\n",
    "        self.zoom_factor = zoom_factor\n",
    "        self.criterion = criterion    \n",
    "        \n",
    "        # resnet101: 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth'\n",
    "        #add condition if we want to remove it (see argument pretrained)\n",
    "        path_to_pretrained = \"./resnet101.pth\"\n",
    "        resnet = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=1000, deep_base=False) #à revoir ?? \n",
    "        resnet.load_state_dict(torch.load(path_to_pretrained), strict=False)\n",
    "\n",
    "        \n",
    "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.conv2, resnet.bn2, resnet.relu, resnet.conv3, resnet.bn3, resnet.relu, resnet.maxpool)\n",
    "        self.layer1, self.layer2, self.layer3, self.layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n",
    "\n",
    "        \n",
    "        #??\n",
    "        for n, m in self.layer3.named_modules():\n",
    "            if 'conv2' in n:\n",
    "                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n",
    "            elif 'downsample.0' in n:\n",
    "                m.stride = (1, 1)\n",
    "        for n, m in self.layer4.named_modules():\n",
    "            if 'conv2' in n:\n",
    "                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n",
    "            elif 'downsample.0' in n:\n",
    "                m.stride = (1, 1)\n",
    "                \n",
    "                \n",
    "        #??\n",
    "        fea_dim = 2048\n",
    "        self.ppm = PPM(fea_dim, int(fea_dim/len(bins)), bins)\n",
    "        fea_dim *= 2\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Conv2d(fea_dim, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "            nn.Conv2d(512, classes, kernel_size=1)\n",
    "        )\n",
    "        if self.training:\n",
    "            self.aux = nn.Sequential(\n",
    "                nn.Conv2d(1024, 256, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout2d(p=dropout),\n",
    "                nn.Conv2d(256, classes, kernel_size=1)\n",
    "            )\n",
    "\n",
    "            \n",
    "            \n",
    "    def forward(self, x, y=None):\n",
    "        x_size = x.size()\n",
    "        assert (x_size[2]-1) % 8 == 0 and (x_size[3]-1) % 8 == 0\n",
    "        h = int((x_size[2] - 1) / 8 * self.zoom_factor + 1)\n",
    "        w = int((x_size[3] - 1) / 8 * self.zoom_factor + 1)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x_tmp = self.layer3(x)\n",
    "        x = self.layer4(x_tmp)\n",
    "        if self.use_ppm:\n",
    "            x = self.ppm(x)\n",
    "        x = self.cls(x)\n",
    "        if self.zoom_factor != 1:\n",
    "            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n",
    "\n",
    "        if self.training:\n",
    "            aux = self.aux(x_tmp)\n",
    "            if self.zoom_factor != 1:\n",
    "                aux = F.interpolate(aux, size=(h, w), mode='bilinear', align_corners=True)\n",
    "            main_loss = self.criterion(x, y)\n",
    "            aux_loss = self.criterion(aux, y)\n",
    "            return x.max(1)[1], main_loss, aux_loss\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch, model, optimizer, aux_weight, device=\"cuda\"):\n",
    "    # TODO fill this function with the training step code\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # TODO retrieve image and label from the batch\n",
    "    x, y = batch\n",
    "    \n",
    "    # TODO move model and code to GPU\n",
    "    model = model.to(device)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    # TODO forward pass\n",
    "    y_hat, main_loss, aux_loss = model(x)\n",
    "    \n",
    "    # TODO loss calculation\n",
    "    loss = main_loss + aux_weight * aux_loss\n",
    "  \n",
    "    # TODO implement backprop and model update\n",
    "    loss.backward() # backpropoagation of gradients\n",
    "    optimizer.step() # update model parameters\n",
    "\n",
    "    # lets also calculate accuracy for fun\n",
    "    # FYI\n",
    "    # .cpu() moves the data back to cpu (if on GPU)\n",
    "    # .detach() removes gradients (we dont need them for accuracy)\n",
    "    # .numpy() converts the tensor to numpy for better handling later\n",
    "    predictions = y_hat.argmax(1).cpu().detach().numpy()\n",
    "    ground_truth = y.cpu().detach().numpy()\n",
    "    \n",
    "    # accuracy is the mean of correct (1) and incorrect (0) classifications\n",
    "    accuracy = (predictions == ground_truth).mean()\n",
    "  \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_dl, model, optimizer, aux_weight):\n",
    "    \n",
    "    # collect some statistics\n",
    "    losses, accuracies = [], []\n",
    "    \n",
    "    for batch in train_dl:\n",
    "        # TODO call training_step\n",
    "        loss, accuracy = training_step(batch, model, optimizer, aux_weight, device = \"cpu\")\n",
    "        \n",
    "        # append statistics\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # return averaged losses and accuracies\n",
    "    return np.stack(losses).mean(), np.stack(accuracies).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "model = PSPNet()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-219f465690ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# TODO: call train_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrainloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainaccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"epoch {epoch}; trainloss {trainloss:.2f}, train accuracy {trainaccuracy*100:.2f}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-63e689eb323b>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(train_dl, model, optimizer, aux_weight)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m# TODO call training_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# append statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-4d337b7eb40f>\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(batch, model, optimizer, aux_weight, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# TODO forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maux_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# TODO loss calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-319cce242c0f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mx_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m8\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m8\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m8\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzoom_factor\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m8\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzoom_factor\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "aux_weight = 0.5\n",
    "\n",
    "stats = []\n",
    "for epoch in range(num_epochs):\n",
    "    # TODO: call train_epoch\n",
    "    trainloss, trainaccuracy = train_epoch(train_loader, model, optimizer, aux_weight)\n",
    "    \n",
    "    print(f\"epoch {epoch}; trainloss {trainloss:.2f}, train accuracy {trainaccuracy*100:.2f}%\")\n",
    "\n",
    "    stats.append({\n",
    "        \"trainloss\":float(trainloss),\n",
    "        \"trainaccuracy\":float(trainaccuracy),\n",
    "        \"epoch\":epoch\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlosses = np.stack([stat[\"trainloss\"] for stat in stats])\n",
    "trainaccuracy = np.stack([stat[\"trainaccuracy\"] for stat in stats])\n",
    "epoch = np.stack([stat[\"epoch\"] for stat in stats])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epoch, trainaccuracy)\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"train accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
