{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloodNet Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Tries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image = Image.open(\"data/image/10165.jpg\")\n",
    "image\n",
    "np_image = np.array(image)\n",
    "print(np_image.shape)\n",
    "#print(np_image)\n",
    "print(np.sum(np_image)/np_image.shape[0]/np_image.shape[1]/np_image.shape[2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import csv\n",
    "#filename = open(\"FloodNet_split_train_valid_test.csv\", 'r')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"FloodNet_split_train_valid_test.csv\", sep=',', header=None, names=[\"Column1\", \"Column2\", \"Column3\"])\n",
    "df1 = df[df[\"Column3\"]=='train']\n",
    "df1[\"Column1\"].map(lambda x: \"data/\" + x)#.iloc[1]\n",
    "\n",
    "list(zip(df1[\"Column1\"], df1[\"Column2\"]))  \n",
    "df1['Column1']\n",
    "\n",
    "for entry in list(df['Column1']):\n",
    "    filepath = 'Data/Image/' + entry\n",
    "    image = Image.open(filepath)\n",
    "    np_image = np.array(image)\n",
    "    if np_image.shape[0] != 3000:\n",
    "        print(entry, np_image.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function\n",
    "#code in this cell from https://albumentations.ai/docs/examples/example_kaggle_salt/\n",
    "def visualize(image, mask, original_image=None, original_mask=None):\n",
    "    fontsize = 18\n",
    "    \n",
    "    if original_image is None and original_mask is None:\n",
    "        f, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "        ax[0].imshow(image)\n",
    "        ax[1].imshow(mask)\n",
    "    else:\n",
    "        f, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "        ax[0, 0].imshow(original_image)\n",
    "        ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
    "        \n",
    "        ax[1, 0].imshow(original_mask)\n",
    "        ax[1, 0].set_title('Original mask', fontsize=fontsize)\n",
    "        \n",
    "        ax[0, 1].imshow(image)\n",
    "        ax[0, 1].set_title('Transformed image', fontsize=fontsize)\n",
    "        \n",
    "        ax[1, 1].imshow(mask)\n",
    "        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloodData(Dataset):\n",
    "\n",
    "    # mapping between label class names and indices\n",
    "    LABEL_CLASSES = {\n",
    "      'background': \t\t  0,\n",
    "      'building-flooded': \t\t\t    1,\n",
    "      'building-non-flooded': \t  2,\n",
    "      'road-flooded': \t\t\t\t      3,\n",
    "      'road-non-flooded': \t\t\t    4,\n",
    "      'water': \t\t\t    5,\n",
    "      'tree':   6,\n",
    "      'vehicle': \t\t\t\t    7,\n",
    "      'pool': \t\t\t\t    8,\n",
    "      'grass': \t\t\t  9\n",
    "    }\n",
    "   \n",
    "\n",
    "    def __init__(self, transforms=None, split='train'):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        \n",
    "        SPLIT = pd.read_csv(\"FloodNet_split_train_valid_test.csv\", sep=',', header=None, names=[\"Column1\", \"Column2\", \"Column3\"])\n",
    "        SPLIT[\"Column1\"] = SPLIT[\"Column1\"].map(lambda x: \"Data/image/\" + x)\n",
    "        SPLIT[\"Column2\"] = SPLIT[\"Column2\"].map(lambda x: \"Data/mask/\" + x)\n",
    "        \n",
    "        splitted_set = SPLIT[SPLIT[\"Column3\"]==split]\n",
    "        \n",
    "        # prepare data\n",
    "        self.data = list(zip(splitted_set[\"Column1\"], splitted_set[\"Column2\"]))                                  # list of tuples of (image path, label class)\n",
    "        \"\"\"\n",
    "        images = np.empty((len(self.data)*3000,len(self.data)*4000,3))\n",
    "        \n",
    "        for i in range(len(self.data)):\n",
    "            images[i] = np.array()\n",
    "            Image.open()\n",
    "        \"\"\" \n",
    "            \n",
    "    #TODO: please provide the remaining functions required for the torch.utils.data.Dataset class.\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        imgName, labelsName = self.data[x]\n",
    "\n",
    "        img = np.array(Image.open(imgName))\n",
    "        labels = np.array(Image.open(labelsName))\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=img, mask=labels)\n",
    "            #code to visualize transformation - uncomment if want to use\n",
    "            #visualize(transformed[\"image\"], transformed[\"mask\"], img, labels)\n",
    "            img = transformed['image']\n",
    "            labels = transformed['mask']\n",
    "        else:\n",
    "            img, labels = img[:3000, :4000,:], labels[:3000, :4000]\n",
    "        \n",
    "        img, labels = torch.tensor(img, dtype=torch.double), torch.tensor(labels)\n",
    "        return img, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [04:22<00:00, 14.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([106.5385, 116.1601,  87.6059], dtype=torch.float64) tensor([53.1838, 49.5204, 53.5829], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# source of code in this cell:\n",
    "# https://www.binarystudy.com/2021/04/how-to-calculate-mean-standard-deviation-images-pytorch.html\n",
    "\n",
    "train_not_transformed_set = FloodData(transforms = None, split = 'train')\n",
    "train_not_transformed_loader = DataLoader(train_not_transformed_set, batch_size = 16)\n",
    "\n",
    "def batch_mean_and_sd(loader):\n",
    "    count = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "    \n",
    "    for images, _  in tqdm(loader):\n",
    "        b, h, w, c = images.shape #batch, height, width, color\n",
    "        nb_pixels = b*h*w\n",
    "        \n",
    "        sum_ = torch.sum(images, dim = [0,1,2])\n",
    "        sum_of_square = torch.sum(torch.square(images), dim = [0,1,2])\n",
    "        \n",
    "        fst_moment = (count*fst_moment+sum_)/(count+nb_pixels)\n",
    "        snd_moment = (count*snd_moment+sum_of_square)/(count+nb_pixels)\n",
    "\n",
    "        count += nb_pixels\n",
    "        \n",
    "    mean, std = fst_moment, torch.sqrt(snd_moment-fst_moment**2)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = batch_mean_and_sd(train_not_transformed_loader)\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = A.Compose([\n",
    "    A.RandomSizedCrop(min_max_height = [1000, 2500], height=713, width=713),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    #A.Blur(blur_limit = 3),\n",
    "    A.RandomRotate90(),\n",
    "    #A.OpticalDistortion(),\n",
    "    #A.GridDistortion(),\n",
    "    #A.Resize(height=713, width=713),\n",
    "    \n",
    "    #Normalization is applied by the formula: img = (img - mean * max_pixel_value) / (std * max_pixel_value)\n",
    "    A.Normalize(mean = mean, std = std, max_pixel_value=1)\n",
    "])\n",
    "\n",
    "transform_val = A.Compose([\n",
    "    A.RandomSizedCrop(min_max_height = [500, 2500], height=713, width=713),\n",
    "    #A.Resize(height=713, width=713),\n",
    "    A.Normalize(mean = mean, std = std, max_pixel_value=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FloodData(transforms = transform_train, split = 'train')\n",
    "val_dataset = FloodData(transforms = transform_val, split = 'valid')\n",
    "test_dataset = FloodData(transforms = transform_val, split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8736, -0.0234, -0.0860],\n",
       "          [ 0.7796, -0.1244, -0.1793],\n",
       "          [ 0.7796, -0.1244, -0.1979],\n",
       "          ...,\n",
       "          [-0.6306, -1.1139, -0.9631],\n",
       "          [-0.5742, -1.0533, -0.8885],\n",
       "          [-0.5742, -1.0129, -0.8698]],\n",
       "\n",
       "         [[ 0.0275, -1.0129, -0.9631],\n",
       "          [ 0.6104, -0.3667, -0.3846],\n",
       "          [ 0.7608, -0.2052, -0.2726],\n",
       "          ...,\n",
       "          [-0.6118, -1.0937, -0.9258],\n",
       "          [-0.5930, -1.0735, -0.9071],\n",
       "          [-0.6118, -1.0735, -0.9258]],\n",
       "\n",
       "         [[-0.2546, -1.3562, -1.2617],\n",
       "          [ 0.6292, -0.3667, -0.4032],\n",
       "          [ 0.5728, -0.4475, -0.4779],\n",
       "          ...,\n",
       "          [-0.4990, -0.9725, -0.8138],\n",
       "          [-0.5554, -1.0331, -0.8698],\n",
       "          [-0.5366, -1.0331, -0.8698]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3283, -0.4879, -0.6458],\n",
       "          [ 0.4599, -0.3667, -0.5712],\n",
       "          [ 0.4599, -0.3667, -0.5712],\n",
       "          ...,\n",
       "          [-0.0665, -0.9321, -0.9444],\n",
       "          [-0.0853, -0.9523, -1.0004],\n",
       "          [-0.0289, -0.8716, -0.9818]],\n",
       "\n",
       "         [[ 0.3471, -0.4475, -0.6458],\n",
       "          [ 0.4599, -0.3465, -0.5899],\n",
       "          [ 0.4975, -0.3465, -0.5899],\n",
       "          ...,\n",
       "          [-0.1229, -0.9725, -1.0378],\n",
       "          [-0.0477, -0.8918, -0.9818],\n",
       "          [ 0.1027, -0.7100, -0.8698]],\n",
       "\n",
       "         [[ 0.3471, -0.4475, -0.6645],\n",
       "          [ 0.4975, -0.3263, -0.5899],\n",
       "          [ 0.4975, -0.3465, -0.5899],\n",
       "          ...,\n",
       "          [-0.1982, -1.0533, -1.1311],\n",
       "          [-0.0477, -0.8918, -1.0004],\n",
       "          [ 0.0463, -0.7908, -0.9258]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 1)\n",
    "next(iter(train_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
