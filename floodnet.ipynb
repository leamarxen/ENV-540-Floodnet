{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 4000, 3)\n",
      "72.27082336111111\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image = Image.open(\"data/image/10165.jpg\")\n",
    "image\n",
    "np_image = np.array(image)\n",
    "print(np_image.shape)\n",
    "#print(np_image)\n",
    "print(np.sum(np_image)/np_image.shape[0]/np_image.shape[1]/np_image.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10828.jpg (3072, 4592, 3)\n",
      "10836.jpg (3072, 4592, 3)\n",
      "10837.jpg (3072, 4592, 3)\n",
      "10840.jpg (3072, 4592, 3)\n",
      "6585.jpg (3072, 4592, 3)\n",
      "6586.jpg (3072, 4592, 3)\n",
      "6589.jpg (3072, 4592, 3)\n",
      "6595.jpg (3072, 4592, 3)\n",
      "6600.jpg (3072, 4592, 3)\n",
      "6614.jpg (3072, 4592, 3)\n",
      "6615.jpg (3072, 4592, 3)\n",
      "6618.jpg (3072, 4592, 3)\n",
      "6623.jpg (3072, 4592, 3)\n",
      "6628.jpg (3072, 4592, 3)\n",
      "6692.jpg (3072, 4592, 3)\n",
      "6693.jpg (3072, 4592, 3)\n",
      "6703.jpg (3072, 4592, 3)\n",
      "6704.jpg (3072, 4592, 3)\n",
      "6706.jpg (3072, 4592, 3)\n",
      "6707.jpg (3072, 4592, 3)\n",
      "6716.jpg (3072, 4592, 3)\n",
      "6994.jpg (3072, 4592, 3)\n",
      "6997.jpg (3072, 4592, 3)\n",
      "7240.jpg (3072, 4592, 3)\n",
      "7243.jpg (3072, 4592, 3)\n",
      "7261.jpg (3072, 4592, 3)\n",
      "7266.jpg (3072, 4592, 3)\n",
      "7267.jpg (3072, 4592, 3)\n",
      "7273.jpg (3072, 4592, 3)\n",
      "7282.jpg (3072, 4592, 3)\n",
      "7289.jpg (3072, 4592, 3)\n",
      "7298.jpg (3072, 4592, 3)\n",
      "7317.jpg (3072, 4592, 3)\n",
      "7321.jpg (3072, 4592, 3)\n",
      "7325.jpg (3072, 4592, 3)\n",
      "7330.jpg (3072, 4592, 3)\n",
      "7331.jpg (3072, 4592, 3)\n",
      "7332.jpg (3072, 4592, 3)\n",
      "7357.jpg (3072, 4592, 3)\n",
      "7362.jpg (3072, 4592, 3)\n",
      "7364.jpg (3072, 4592, 3)\n",
      "7372.jpg (3072, 4592, 3)\n",
      "7405.jpg (3072, 4592, 3)\n",
      "7414.jpg (3072, 4592, 3)\n",
      "7418.jpg (3072, 4592, 3)\n",
      "7428.jpg (3072, 4592, 3)\n",
      "7434.jpg (3072, 4592, 3)\n",
      "7463.jpg (3072, 4592, 3)\n",
      "7466.jpg (3072, 4592, 3)\n",
      "7473.jpg (3072, 4592, 3)\n",
      "7485.jpg (3072, 4592, 3)\n",
      "7488.jpg (3072, 4592, 3)\n",
      "7523.jpg (3072, 4592, 3)\n",
      "7524.jpg (3072, 4592, 3)\n",
      "7546.jpg (3072, 4592, 3)\n",
      "7556.jpg (3072, 4592, 3)\n",
      "7571.jpg (3072, 4592, 3)\n",
      "7587.jpg (3072, 4592, 3)\n",
      "7594.jpg (3072, 4592, 3)\n"
     ]
    }
   ],
   "source": [
    "#import csv\n",
    "#filename = open(\"FloodNet_split_train_valid_test.csv\", 'r')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"FloodNet_split_train_valid_test.csv\", sep=',', header=None, names=[\"Column1\", \"Column2\", \"Column3\"])\n",
    "df1 = df[df[\"Column3\"]=='train']\n",
    "df1[\"Column1\"].map(lambda x: \"data/\" + x)#.iloc[1]\n",
    "\n",
    "list(zip(df1[\"Column1\"], df1[\"Column2\"]))  \n",
    "df1['Column1']\n",
    "\n",
    "for entry in list(df['Column1']):\n",
    "    filepath = 'Data/Image/' + entry\n",
    "    image = Image.open(filepath)\n",
    "    np_image = np.array(image)\n",
    "    if np_image.shape[0] != 3000:\n",
    "        print(entry, np_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "class FloodData(Dataset):\n",
    "\n",
    "    # mapping between label class names and indices\n",
    "    LABEL_CLASSES = {\n",
    "      'background': \t\t  0,\n",
    "      'building-flooded': \t\t\t    1,\n",
    "      'building-non-flooded': \t  2,\n",
    "      'road-flooded': \t\t\t\t      3,\n",
    "      'road-non-flooded': \t\t\t    4,\n",
    "      'water': \t\t\t    5,\n",
    "      'tree':   6,\n",
    "      'vehicle': \t\t\t\t    7,\n",
    "      'pool': \t\t\t\t    8,\n",
    "      'grass': \t\t\t  9\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self, transforms=None, split='train'):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        \n",
    "        SPLIT = pd.read_csv(\"FloodNet_split_train_valid_test.csv\", sep=',', header=None, names=[\"Column1\", \"Column2\", \"Column3\"])\n",
    "        SPLIT[\"Column1\"] = SPLIT[\"Column1\"].map(lambda x: \"Data/image/\" + x)\n",
    "        SPLIT[\"Column2\"] = SPLIT[\"Column2\"].map(lambda x: \"Data/mask/\" + x)\n",
    "        \n",
    "        splitted_set = SPLIT[SPLIT[\"Column3\"]==split]\n",
    "        \n",
    "        # prepare data\n",
    "        self.data = list(zip(splitted_set[\"Column1\"], splitted_set[\"Column2\"]))                                  # list of tuples of (image path, label class)\n",
    "        \"\"\"\n",
    "        images = np.empty((len(self.data)*3000,len(self.data)*4000,3))\n",
    "        \n",
    "        for i in range(len(self.data)):\n",
    "            images[i] = np.array()\n",
    "            Image.open()\n",
    "        \"\"\" \n",
    "            \n",
    "    #TODO: please provide the remaining functions required for the torch.utils.data.Dataset class.\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        imgName, labelsName = self.data[x]\n",
    "\n",
    "        img = np.array(Image.open(imgName))\n",
    "        labels = np.array(Image.open(labelsName))\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=img, mask=labels)\n",
    "            img = transformed['image']\n",
    "            labels = transformed['mask']\n",
    "        else:\n",
    "            img, labels = img[:3000, :4000,:], labels[:3000, :4000]\n",
    "        \n",
    "        img, labels = torch.tensor(img, dtype=torch.int32), torch.tensor(labels)\n",
    "        return img, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [04:28<00:00, 14.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([106.5385, 116.1601,  87.6059]) tensor([53.1837, 49.5205, 53.5828])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# source of code in this cell:\n",
    "# https://www.binarystudy.com/2021/04/how-to-calculate-mean-standard-deviation-images-pytorch.html\n",
    "\n",
    "train_not_transformed_set = FloodData(transforms = None, split = 'train')\n",
    "train_not_transformed_loader = DataLoader(train_not_transformed_set, batch_size = 16)\n",
    "\n",
    "def batch_mean_and_sd(loader):\n",
    "    count = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "    \n",
    "    for images, _  in tqdm(loader):\n",
    "        b, h, w, c = images.shape #batch, height, width, color\n",
    "        nb_pixels = b*h*w\n",
    "        \n",
    "        sum_ = torch.sum(images, dim = [0,1,2])\n",
    "        sum_of_square = torch.sum(torch.square(images), dim = [0,1,2])\n",
    "        \n",
    "        fst_moment = (count*fst_moment+sum_)/(count+nb_pixels)\n",
    "        snd_moment = (count*snd_moment+sum_of_square)/(count+nb_pixels)\n",
    "\n",
    "        count += nb_pixels\n",
    "        \n",
    "    mean, std = fst_moment, torch.sqrt(snd_moment-fst_moment**2)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = batch_mean_and_sd(train_not_transformed_loader)\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "\n",
    "transform_train = A.Compose([\n",
    "    A.RandomSizedCrop(min_max_height = [1000, 2500], height=3000, width=4000),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ShiftScaleRotate(p=0.5),\n",
    "    #A.Blur(blur_limit = 3),\n",
    "    A.RandomRotate90(),\n",
    "    #A.OpticalDistortion(),\n",
    "    #A.GridDistortion(),\n",
    "    A.Normalize(mean = mean, std = std),\n",
    "])\n",
    "\n",
    "transform_val = A.Compose([\n",
    "    A.RandomSizedCrop(min_max_height = [1000, 2500], height=3000, width=4000),\n",
    "    A.Normalize(mean = mean, std = std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FloodData(transforms = transform_train, split = 'train')\n",
    "val_dataset = FloodData(transforms = transform_val, split = 'valid')\n",
    "test_dataset = FloodData(transforms = transform_val, split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         ...,\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1]],\n",
       "\n",
       "        [[-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         ...,\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1]],\n",
       "\n",
       "        [[-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         ...,\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         ...,\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1]],\n",
       "\n",
       "        [[-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         ...,\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1]],\n",
       "\n",
       "        [[-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         ...,\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1],\n",
       "         [-1, -2, -1]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 16)\n",
    "next(iter(train_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
